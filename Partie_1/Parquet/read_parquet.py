from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg
from io import StringIO
import sys
from datetime import datetime

spark = SparkSession.builder \
    .appName("ReadParquetExample") \
    .getOrCreate()

df = spark.read.parquet("/app/Parquet/sample_output_parquet")

log_path = "/app/logs/statistiques_parquet.txt"

# Rediriger la sortie vers un buffer pour log + console
buffer = StringIO()
original_stdout = sys.stdout
sys.stdout = buffer

# üìä Affichage des donn√©es
print("\n Exemple de donn√©es charg√©es depuis les fichiers Parquet :\n")
df.show(10, truncate=False)

print("\nStatistiques sur les donn√©es :")

# Statistiques principales
total = df.count()
print(f"- Nombre total de documents : {total}")

communes_uniques = df.select("commune").distinct().count()
print(f"- Nombre de communes diff√©rentes : {communes_uniques}")

valeur_moyenne = df.select(avg("valeur_fonciere")).first()[0]
print(f"- Moyenne de la valeur fonci√®re : {valeur_moyenne:.2f} ‚Ç¨")

print("\n Top 5 des communes avec le plus de ventes :")
top_communes = df.groupBy("commune") \
    .count() \
    .orderBy(col("count").desc()) \
    .limit(5)
top_communes.show(truncate=False)

# √âcriture dans le fichier
sys.stdout = original_stdout
with open(log_path, "w", encoding="utf-8") as f:
    f.write("=" * 60 + "\n")
    f.write(f"Rapport g√©n√©r√© le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(buffer.getvalue())

# ‚úÖ Impression dans la console pour v√©rif
print(buffer.getvalue())

spark.stop()
