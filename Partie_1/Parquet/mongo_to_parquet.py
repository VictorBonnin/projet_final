from pyspark.sql import SparkSession
import time

# Cr√©ation de la session Spark
spark = SparkSession.builder \
    .appName("MongoToParquetSample") \
    .config("spark.mongodb.read.connection.uri", "mongodb://mongodb:27017/projet_final.streaming_resultats") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

start = time.time()
print("üöÄ Lecture de l'√©chantillon depuis MongoDB...")

# Lecture brute (non partitionn√©e, volontaire ici car sample)
df = spark.read \
    .format("com.mongodb.spark.sql.DefaultSource") \
    .option("uri", "mongodb://mongodb:27017") \
    .option("database", "projet_final") \
    .option("collection", "streaming_resultats") \
    .load()

# Cr√©ation d'un √©chantillon al√©atoire de 20 000 lignes
sample_df = df.sample(withReplacement=False, fraction=0.1, seed=42).limit(20000)

# Aper√ßu
print("\nüìä Aper√ßu de l'√©chantillon :")
sample_df.show(5)
print(f"\nüìå Taille de l'√©chantillon : {sample_df.count()}")
sample_df.printSchema()

# √âcriture Parquet
output_path = "/app/Parquet/sample_output_parquet"
sample_df.write.mode("overwrite").option("compression", "snappy").parquet(output_path)

print(f"\n‚úÖ √âchantillon export√© avec succ√®s au format Parquet dans : {output_path}")
print(f"‚è±Ô∏è Temps total : {time.time() - start:.2f} secondes")

spark.stop()